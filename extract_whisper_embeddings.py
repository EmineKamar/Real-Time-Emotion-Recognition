import os
import numpy as np
import torch
import torchaudio
from datasets import load_dataset
from tqdm import tqdm
import pandas as pd
from transformers import AutoProcessor, AutoModel

# GPU kontrolÃ¼
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"âœ… Cihaz: {device}")

# Whisper modeli ve iÅŸlemcisi
processor = AutoProcessor.from_pretrained("openai/whisper-base")
model = AutoModel.from_pretrained("openai/whisper-base", trust_remote_code=True).to(device)
model.eval()

# Dataset yÃ¼kle
dataset = load_dataset("myleslinder/crema-d", trust_remote_code=True)["train"]

# Duygu etiket eÅŸlemesi
label_to_emotion = {
    0: "NEU",  # Neutral
    1: "HAP",  # Happy
    2: "SAD",  # Sad
    3: "ANG",  # Angry
    4: "FEA",  # Fear
    5: "DIS",  # Disgust
}

# Veri hazÄ±rlama
features = []
labels = []

print("ðŸŽ§ Embedding Ã§Ä±karÄ±mÄ± baÅŸlÄ±yor...")
for sample in tqdm(dataset):
    audio = sample["audio"]
    waveform = torch.tensor(audio["array"]).float().unsqueeze(0)
    sampling_rate = audio["sampling_rate"]

    # Whisper iÃ§in yeniden Ã¶rnekleme
    if sampling_rate != 16000:
        waveform = torchaudio.functional.resample(waveform, sampling_rate, 16000)

    # Whisper input
    inputs = processor(waveform.squeeze().numpy(), sampling_rate=16000, return_tensors="pt")
    input_values = inputs.input_values.to(device)

    with torch.no_grad():
        out = model(input_values)
        embedding = out.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()  # [embedding_dim]

    features.append(embedding)
    labels.append(label_to_emotion[sample["label"]])

# KayÄ±t
os.makedirs("data", exist_ok=True)
np.save("data/whisper_embeddings.npy", np.array(features))

df = pd.DataFrame({
    "label": labels
})
df.to_csv("data/labels.csv", index=False)

print("âœ… Embedding Ã§Ä±karÄ±mÄ± tamamlandÄ±. Kaydedilenler:")
print("- data/whisper_embeddings.npy")
print("- data/labels.csv")
